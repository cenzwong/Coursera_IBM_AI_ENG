{"cells":[{"cell_type":"markdown","source":["d-sandbox\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 400px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dafff6c9-2602-4ab2-9a79-7267a9d86a20"}}},{"cell_type":"markdown","source":["# Reader & Writer\n1. Read from CSV files\n1. Read from JSON files\n1. Write DataFrame to files\n1. Write DataFrame to tables\n1. Write DataFrame to a Delta table\n\n##### Methods\n- DataFrameReader (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">Python</a>/<a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameReader.html\" target=\"_blank\">Scala</a>): `csv`, `json`, `option`, `schema`\n- DataFrameWriter (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#input-and-output\" target=\"_blank\">Python</a>/<a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/DataFrameWriter.html\" target=\"_blank\">Scala</a>): `mode`, `option`, `parquet`, `format`, `saveAsTable`\n- StructType (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.types.StructType.html#pyspark.sql.types.StructType\" target=\"_blank\">Python</a>/<a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/StructType.html\" target=\"_blank\" target=\"_blank\">Scala</a>): `toDDL`\n\n##### Spark Types\n- Types (<a href=\"https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql.html#data-types\" target=\"_blank\">Python</a>/<a href=\"https://spark.apache.org/docs/latest/api/scala/org/apache/spark/sql/types/index.html\" target=\"_blank\">Scala</a>): `ArrayType`, `DoubleType`, `IntegerType`, `LongType`, `StringType`, `StructType`, `StructField`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e1bc10f8-762e-4185-ad59-2d97ba5cfb46"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Setup"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b6515915-1182-4d1b-8bcb-b426476a710d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Datasets mounted and student environment set up","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Datasets mounted and student environment set up"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from CSV files\nRead from CSV with DataFrameReader's `csv` method and the following options:\n\nTab separator, use first line as header, infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"986091fe-d035-4243-9057-d5e67a249a3b"}}},{"cell_type":"code","source":["usersCsvPath = \"/mnt/training/ecommerce/users/users-500k.csv\"\n\nusersDF = (spark.read\n  .option(\"sep\", \"\\t\")\n  .option(\"header\", True)\n  .option(\"inferSchema\", True)\n  .csv(usersCsvPath))\n\nusersDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1498aa20-1a1a-4ded-8081-b34e30c0f890"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- user_id: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- email: string (nullable = true)\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Manually define the schema by creating a `StructType` with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3360d789-9eab-4716-9a8d-efccd6eccb9a"}}},{"cell_type":"code","source":["from pyspark.sql.types import LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n  StructField(\"user_id\", StringType(), True),\n  StructField(\"user_first_touch_timestamp\", LongType(), True),\n  StructField(\"email\", StringType(), True)\n])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"698114b8-5800-4fab-88ae-843ed88c1814"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Read from CSV using this user-defined schema instead of inferring schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"f346f359-0442-43ca-9cb3-8614979b0b45"}}},{"cell_type":"code","source":["usersDF = (spark.read\n  .option(\"sep\", \"\\t\")\n  .option(\"header\", True)\n  .schema(userDefinedSchema)\n  .csv(usersCsvPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"21296fb9-b8f9-449c-a0ea-193c3e3165d2"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["Alternatively, define the schema using a DDL formatted string."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2052d59b-c530-446c-8213-3b22f142f4ea"}}},{"cell_type":"code","source":["DDLSchema = \"user_id string, user_first_touch_timestamp long, email string\"\n\nusersDF = (spark.read\n  .option(\"sep\", \"\\t\")\n  .option(\"header\", True)\n  .schema(DDLSchema)\n  .csv(usersCsvPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e279c737-938d-4d59-bf08-93540966bb24"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Read from JSON files\n\nRead from JSON with DataFrameReader's `json` method and the infer schema option"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"dbf57703-a430-4dd4-a2c1-a5346a2cfa6a"}}},{"cell_type":"code","source":["eventsJsonPath = \"/mnt/training/ecommerce/events/events-500k.json\"\n\neventsDF = (spark.read\n  .option(\"inferSchema\", True)\n  .json(eventsJsonPath))\n\neventsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"15b985f0-57b5-4e0c-b46f-bc93b454ba03"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["root\n |-- device: string (nullable = true)\n |-- ecommerce: struct (nullable = true)\n |    |-- purchase_revenue_in_usd: double (nullable = true)\n |    |-- total_item_quantity: long (nullable = true)\n |    |-- unique_items: long (nullable = true)\n |-- event_name: string (nullable = true)\n |-- event_previous_timestamp: long (nullable = true)\n |-- event_timestamp: long (nullable = true)\n |-- geo: struct (nullable = true)\n |    |-- city: string (nullable = true)\n |    |-- state: string (nullable = true)\n |-- items: array (nullable = true)\n |    |-- element: struct (containsNull = true)\n |    |    |-- coupon: string (nullable = true)\n |    |    |-- item_id: string (nullable = true)\n |    |    |-- item_name: string (nullable = true)\n |    |    |-- item_revenue_in_usd: double (nullable = true)\n |    |    |-- price_in_usd: double (nullable = true)\n |    |    |-- quantity: long (nullable = true)\n |-- traffic_source: string (nullable = true)\n |-- user_first_touch_timestamp: long (nullable = true)\n |-- user_id: string (nullable = true)\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["Read data faster by creating a `StructType` with the schema names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c0c40e9a-2ea0-4167-a8db-85a550933b2f"}}},{"cell_type":"code","source":["from pyspark.sql.types import ArrayType, DoubleType, IntegerType, LongType, StringType, StructType, StructField\n\nuserDefinedSchema = StructType([\n  StructField(\"device\", StringType(), True),\n  StructField(\"ecommerce\", StructType([\n    StructField(\"purchaseRevenue\", DoubleType(), True),\n    StructField(\"total_item_quantity\", LongType(), True),\n    StructField(\"unique_items\", LongType(), True)\n  ]), True),\n  StructField(\"event_name\", StringType(), True),\n  StructField(\"event_previous_timestamp\", LongType(), True),\n  StructField(\"event_timestamp\", LongType(), True),\n  StructField(\"geo\", StructType([\n    StructField(\"city\", StringType(), True),\n    StructField(\"state\", StringType(), True)\n  ]), True),\n  StructField(\"items\", ArrayType(\n    StructType([\n      StructField(\"coupon\", StringType(), True),\n      StructField(\"item_id\", StringType(), True),\n      StructField(\"item_name\", StringType(), True),\n      StructField(\"item_revenue_in_usd\", DoubleType(), True),\n      StructField(\"price_in_usd\", DoubleType(), True),\n      StructField(\"quantity\", LongType(), True)\n    ])\n  ), True),\n  StructField(\"traffic_source\", StringType(), True),\n  StructField(\"user_first_touch_timestamp\", LongType(), True),\n  StructField(\"user_id\", StringType(), True)\n])\n\neventsDF = (spark.read\n  .schema(userDefinedSchema)\n  .json(eventsJsonPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"960d2f5c-560d-4c7d-b2c0-8a2b3088edec"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["You can use the `StructType` Scala method `toDDL` to have a DDL-formatted string created for you.\n\nIn Python, create a Scala cell to create the string to copy and paste."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"055a02e3-c75e-4bf5-8213-90c6cab08545"}}},{"cell_type":"code","source":["%scala\nspark.read.parquet(\"/mnt/training/ecommerce/events/events.parquet\").schema.toDDL"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c9a094fc-3f09-46e1-818c-03152113b06d"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<div class=\"ansiout\">res1: String = `device` STRING,`ecommerce` STRUCT&lt;`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT&gt;,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT&lt;`city`: STRING, `state`: STRING&gt;,`items` ARRAY&lt;STRUCT&lt;`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT&gt;&gt;,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\n</div>","removedWidgets":[],"addedWidgets":{},"metadata":{"isDbfsCommandResult":false},"type":"html","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style scoped>\n  .ansiout {\n    display: block;\n    unicode-bidi: embed;\n    white-space: pre-wrap;\n    word-wrap: break-word;\n    word-break: break-all;\n    font-family: \"Source Code Pro\", \"Menlo\", monospace;;\n    font-size: 13px;\n    color: #555;\n    margin-left: 4px;\n    line-height: 19px;\n  }\n</style>\n<div class=\"ansiout\">res1: String = `device` STRING,`ecommerce` STRUCT&lt;`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT&gt;,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT&lt;`city`: STRING, `state`: STRING&gt;,`items` ARRAY&lt;STRUCT&lt;`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT&gt;&gt;,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\n</div>"]},"transient":null}],"execution_count":0},{"cell_type":"code","source":["DDLSchema = \"`device` STRING,`ecommerce` STRUCT<`purchase_revenue_in_usd`: DOUBLE, `total_item_quantity`: BIGINT, `unique_items`: BIGINT>,`event_name` STRING,`event_previous_timestamp` BIGINT,`event_timestamp` BIGINT,`geo` STRUCT<`city`: STRING, `state`: STRING>,`items` ARRAY<STRUCT<`coupon`: STRING, `item_id`: STRING, `item_name`: STRING, `item_revenue_in_usd`: DOUBLE, `price_in_usd`: DOUBLE, `quantity`: BIGINT>>,`traffic_source` STRING,`user_first_touch_timestamp` BIGINT,`user_id` STRING\"\n\neventsDF = (spark.read\n  .schema(DDLSchema)\n  .json(eventsJsonPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"953b2157-41a5-47e3-96a2-1472e8e784d5"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Write DataFrames to files\n\nWrite `usersDF` to parquet with DataFrameWriter's `parquet` method and the following configurations:\n\nSnappy compression, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"921446e4-58b3-4958-8a79-e5e87af43864"}}},{"cell_type":"code","source":["usersOutputPath = workingDir + \"/users.parquet\"\n\n(usersDF.write\n  .option(\"compression\", \"snappy\")\n  .mode(\"overwrite\")\n  .parquet(usersOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66468e1e-1d6a-4f7f-b40d-5fb89f40f618"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n### ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Write DataFrames to tables\n\nWrite `eventsDF` to a table using the DataFrameWriter method `saveAsTable`\n\n<img alt=\"Side Note\" title=\"Side Note\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.05em; transform:rotate(15deg)\" src=\"https://files.training.databricks.com/static/images/icon-note.webp\"/> This creates a global table, unlike the local view created by the DataFrame method `createOrReplaceTempView`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"5cef0519-0b9f-43b7-89e7-de8ae617fba4"}}},{"cell_type":"code","source":["eventsDF.write.mode(\"overwrite\").saveAsTable(\"events_p\")"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"b54b2dbc-1ea9-429a-b935-77544fb3ac05"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["This table was saved in the database created for you in classroom setup. See database name printed below."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"52db859d-6328-43e4-a0eb-c33bba8b25a2"}}},{"cell_type":"code","source":["print(databaseName)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"a5daf4ce-3cc4-4b30-b358-b283067261c7"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"cenzwongekimetricscom_spark_programming_1_4_reader___writer_py\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["cenzwongekimetricscom_spark_programming_1_4_reader___writer_py\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### ![Delta Lake Tiny Logo](https://pages.databricks.com/rs/094-YMS-629/images/delta-lake-tiny-logo.png) Best Practice: Write Results to a Delta Table\n\nIn almost all cases, the best practice is to use <a href=\"https://delta.io/\" target=\"_blank\">Delta Lake</a>, especially whenever the data will be referenced from a Databricks Workspace. Data in Delta tables is stored in Parquet format.\n\nWrite `eventsDF` to Delta with DataFrameWriter's `save` method and the following configurations:\n\nDelta format, overwrite mode"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"99be2314-62a9-44f9-bd06-334043e54231"}}},{"cell_type":"code","source":["eventsOutputPath = workingDir + \"/delta/events\"\n\n(eventsDF.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(eventsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6da788c-9388-4a41-988e-8dcdf97a1ad9"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["## ![Spark Logo Tiny](https://files.training.databricks.com/images/105/logo_spark_tiny.png) Ingesting Data Lab\n\nRead in CSV files containing products data.\n1. Read with infer schema\n2. Read with user-defined schema\n3. Read with DDL formatted string\n4. Write to Delta"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e6432af9-565d-4acf-9a40-8d2b822c9526"}}},{"cell_type":"markdown","source":["### 1. Read with infer schema\n- View the first CSV file using DBUtils method `fs.head` with the filepath provided in the variable `singleProductCsvFilePath`\n- Create `productsDF` by reading from CSV files located in the filepath provided in the variable `productsCsvPath`\n  - Configure options to use first line as header and infer schema"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6158c2a1-ed52-490d-a7bc-28da965e9815"}}},{"cell_type":"code","source":["# ANSWER\nsingleProductCsvFilePath = \"/mnt/training/ecommerce/products/products.csv/part-00000-tid-1663954264736839188-daf30e86-5967-4173-b9ae-d1481d3506db-2367-1-c000.csv\"\nprint(dbutils.fs.head(singleProductCsvFilePath))\n\nproductsCsvPath = \"/mnt/training/ecommerce/products/products.csv\"\n\nproductsDF = (spark.read\n  .option(\"header\", True)\n  .option(\"inferSchema\", True)\n  .csv(productsCsvPath))\n\nproductsDF.printSchema()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8c963fa2-de7f-4d46-8404-6358ec7100ef"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"item_id,name,price\nM_PREM_Q,Premium Queen Mattress,1795.0\nM_STAN_F,Standard Full Mattress,945.0\nM_PREM_F,Premium Full Mattress,1695.0\n\nroot\n |-- item_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- price: double (nullable = true)\n\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["item_id,name,price\nM_PREM_Q,Premium Queen Mattress,1795.0\nM_STAN_F,Standard Full Mattress,945.0\nM_PREM_F,Premium Full Mattress,1695.0\n\nroot\n |-- item_id: string (nullable = true)\n |-- name: string (nullable = true)\n |-- price: double (nullable = true)\n\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"899a3022-8a4c-4382-9e4d-b1da353e7e1c"}}},{"cell_type":"code","source":["assert(productsDF.count() == 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58bac1e4-7c56-42a0-8e51-8b3b0235d98f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 2. Read with user-defined schema\nDefine schema by creating a `StructType` with column names and data types"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa737e9a-9ae1-436c-a48a-86440df58af8"}}},{"cell_type":"code","source":["# ANSWER\nuserDefinedSchema = StructType([\n  StructField(\"item_id\", StringType(), True),\n  StructField(\"name\", StringType(), True),\n  StructField(\"price\", DoubleType(), True)\n])\n\nproductsDF2 = (spark.read\n  .option(\"header\", True)\n  .schema(userDefinedSchema)\n  .csv(productsCsvPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8ad7ab39-7a86-4811-a6d2-cb91e33fb5b3"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"293a79d5-f3b5-47f1-9256-e5df15ae30c2"}}},{"cell_type":"code","source":["assert(userDefinedSchema.fieldNames() == [\"item_id\", \"name\", \"price\"])"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e0bd559d-ffbc-405e-8951-d7158875b2b7"}},"outputs":[],"execution_count":0},{"cell_type":"code","source":["from pyspark.sql import Row\n\nexpected1 = Row(item_id=\"M_STAN_Q\", name=\"Standard Queen Mattress\", price=1045.0)\nresult1 = productsDF2.first()\n\nassert(expected1 == result1)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"94299a07-9bb5-4029-801d-6fe1b1292ca0"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 3. Read with DDL formatted string"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c554d1df-323f-4b49-870a-f5303fda8681"}}},{"cell_type":"code","source":["# ANSWER\nDDLSchema = \"`item_id` STRING,`name` STRING,`price` DOUBLE\"\n\nproductsDF3 = (spark.read\n  .option(\"header\", True)\n  .schema(DDLSchema)\n  .csv(productsCsvPath))"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"16ed5470-fc63-47c2-871e-01b4dfa4a9ac"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c781264e-ccc1-46dd-8ec1-9215a1401aa5"}}},{"cell_type":"code","source":["assert(productsDF3.count() == 12)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"440b0314-189b-4209-8fb3-b7b527b4a37b"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### 4. Write to Delta\nWrite `productsDF` to the filepath provided in the variable `productsOutputPath`"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"66c06489-7071-41e9-bcd2-abc115b7cec3"}}},{"cell_type":"code","source":["# ANSWER\nproductsOutputPath = workingDir + \"/delta/products\"\n(productsDF.write\n  .format(\"delta\")\n  .mode(\"overwrite\")\n  .save(productsOutputPath)\n)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"cd394517-99a8-45be-add2-4678b3adaecc"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n##### <img alt=\"Best Practice\" title=\"Best Practice\" style=\"vertical-align: text-bottom; position: relative; height:1.75em; top:0.3em\" src=\"https://files.training.databricks.com/static/images/icon-blue-ribbon.svg\"/> Check your work"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"3228d62d-4121-4297-8703-499e0f263a35"}}},{"cell_type":"code","source":["assert(len(dbutils.fs.ls(productsOutputPath)) == 5)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"d8a34bd8-a6fa-4ab3-86e8-2c32a8aa745f"}},"outputs":[],"execution_count":0},{"cell_type":"markdown","source":["### Clean up classroom"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"1e943c96-a27f-4e5c-854a-5af03b6468d9"}}},{"cell_type":"code","source":["%run ./Includes/Classroom-Cleanup\n"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"c5427730-a139-4302-a97c-7171dc9a5233"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Dropped database and removed files in working directory","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["Dropped database and removed files in working directory"]},"transient":null}],"execution_count":0}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"1.4 Reader & Writer","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2610767701418667}},"nbformat":4,"nbformat_minor":0}
