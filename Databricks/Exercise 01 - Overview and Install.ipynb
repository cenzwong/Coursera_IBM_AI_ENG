{"cells":[{"cell_type":"markdown","source":["-sandbox\n\n<div style=\"text-align: center; line-height: 0; padding-top: 9px;\">\n  <img src=\"https://databricks.com/wp-content/uploads/2018/03/db-academy-rgb-1200px.png\" alt=\"Databricks Learning\" style=\"width: 600px\">\n</div>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"421ca924-e454-4adb-bf87-55811cc60efb"}}},{"cell_type":"markdown","source":["# Exercise #1 - Project Overview & Dataset Install\n\nThe capstone project aims to assess rudimentary skills as it relates to the Apache Spark and DataFrame APIs.\n\nThe approach taken here assumes that you are familiar with and have some experience with the following entities:\n* **`SparkContext`**\n* **`SparkSession`**\n* **`DataFrame`**\n* **`DataFrameReader`**\n* **`DataFrameWriter`**\n* The various functions found in the module **`pyspark.sql.functions`**\n\nThroughout this project, you will be given specific instructions and it is our expectation that you will be able to complete these instructions drawing on your existing knowledge as well as other sources such as the <a href=\"https://spark.apache.org/docs/latest/api.html\" target=\"_blank\">Spark API Documentation</a>.\n\nAfter reviewing the project, the datasets and the various exercises, we will install the<br/>\ndatasets into your Databricks workspace so that you may proceed with this capstone project."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bfa8fcf8-5dd8-429b-a039-ad21ab07a733"}}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Project Overview</h2>\n* The Project - an introduction to this project\n* The Data - an introduction to this project's datasets\n* The Exercises - an overview of the various exercises in this project"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"39cd4eff-1c10-4421-b130-57d7a2d9401d"}}},{"cell_type":"markdown","source":["### The Project\n\nThe idea behind this project is to ingest data from a purchasing system and load it into a data lake for further analysis. \n\nEach exercise is broken up into smaller steps, or milestones.\n\nAfter every milestone, we have provided a \"reality check\" to help ensure that you are progressing as expected.\n\nPlease note, because each exercise builds on the previous, it is essential to complete all exercises and ensure their \"reality checks\" pass, before moving on to the next exercise.\n\nAs the last exercise of this project, we will use this data we loaded to answer some simple business questions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"48a0e16b-42c4-45b5-865e-c6278e4f4b2e"}}},{"cell_type":"markdown","source":["### The Data\nThe raw data comes in three forms:\n\n1. Orders that were processed in 2017, 2018 and 2019.\n  * For each year a separate batch (or backup) of that year's orders was produced\n  * The format of all three files are similar, but were not produced exactly the same:\n    * 2017 is in a fixed-width text file format\n    * 2018 is tab-separated text file\n    * 2019 is comma-separated text file\n  * Each order consists for four main data points:\n    0. The order - the highest level aggregate\n    0. The line items - the individual products purchased in the order\n    0. The sales reps - the person placing the order\n    0. The customer - the person who purchased the items and where it was shipped.\n  * All three batches are consistent in that there is one record per line item creating a significant amount of duplicated data across orders, reps and customers.\n  * All entities are generally referenced by an ID, such as order_id, customer_id, etc.\n  \n2. All products to be sold by this company (SKUs) are represented in a single XML file\n\n3. In 2020, the company switched systems and now lands a single JSON file in cloud storage for every order received.\n  * These orders are simplified versions of the batched data fro 2017-2019 and includes only the order's details, the line items, and the correlating ids\n  * The sales reps's data is no longer represented in conjunction with an order"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"58550220-d62b-41b0-914b-61892f624468"}}},{"cell_type":"markdown","source":["### The Exercises\n\n* In **Exercise #1**, (this notebook) we introduce the registration procedure, the installation of our datasets and the reality-checks meant to aid you in your progress thought this capstone project.\n\n* In **Exercise #2**, we will ingest the batch data for 2017-2019, combine them into a single dataset for future processing.\n\n* In **Exercise #3**, we will take the unified batch data from **Exercise #2**, clean it, and extract it into three new datasets: Orders, Line Items and Sales Reps. The customer data, for the sake of simplicity, will not be broken out and left with the orders.\n\n* In **Exercise #4**, we will ingest the XML document containing all the projects, and combine it with the Line Items to create yet another dataset, Product Line Items.\n\n* In **Exercise #5**, we will begin processing the stream of orders for 2020, appending that stream of data to the existing datasets as necessary.\n\n* In **Exercise #6**, we will use all of our new datasets to answer a handful of business questions.\n\n* In **Exercise #7**, we provide final instructions for submitting your capstone project."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"6cb0f17b-8ec4-4173-af88-7b644e0ea019"}}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #1 - Install Datasets</h2>\n\nThe datasets for this project are stored in a public object store.\n\nThey need to be downloaded and installed into your Databricks workspace before proceeding with this project.\n\nBut before doing that, we need to configure a cluster appropriate for this project.\n\n**In this step you will need to:**\n1. Configure the cluster (see specific instructions below)\n2. Attach this notebook to your cluster\n3. Specify your Registration ID\n4. Run the setup notebook for this exercise\n5. Install the datasets\n6. Run the reality check to verify the datasets were correctly installed\n\nNote: These steps represent the basic pattern used by each exercise in this capstone project"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fe16c304-317a-43f3-98d2-acfccd3a22df"}}},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Setup Exercise #1</h2>\n\nTo get started, we first need to configure your Registration ID and then run the setup notebook."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8a5a3f0e-fbda-47a3-8ee7-6cffd97b537d"}}},{"cell_type":"markdown","source":["### Setup - Create A Cluster\n\n#### Databricks Community Edition\n\nThis Capstone project was designed to work with Databricks Runtime Version (DBR) 9.1 LTS and the Databricks Community Edition's (CE) default cluster configuration. \n\nWhen working in CE, start a default cluster, specify **DBR 9.1 LTS**, and then proceede with the next step. \n\n#### Other than Community Edition (MSA, AWS or GCP)\n\nThis capstone project was designed to work with a small, single-node cluster when not using CE. When configuring your cluster, please specify the following:\n\n* DBR: **9.1 LTS** \n* Cluster Mode: **Single Node**\n* Node Type: \n  * for Microsoft Azure - **Standard_E4ds_v4**\n  * for Amazon Web Services - **i3.xlarge** \n  * for Google Cloud Platform - **n1-highmem-4** \n\nPlease feel free to use the Community Edition if the recomended node types are not available."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"fa237c09-9c6e-42da-b319-a4ccddbb57d6"}}},{"cell_type":"markdown","source":["### Setup - Run the exercise setup\n\nRun the following cell to setup this exercise, declaring exercise-specific variables and functions."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"e9b2ed0e-a8f7-4b69-8f85-d20ab274578f"}}},{"cell_type":"code","source":["%run ./_includes/Setup-Exercise-01"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"8d3ef1ca-b777-4f48-bae5-d3a2857f96ee"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<html><body><table style=\"width:100%\">\n            <p style=\"font-size:16px\">The following variables and functions have been defined for you.<br/>Please refer to them in the following instructions.</p><tr><th style=\"border-bottom:1px solid #CDCDCD; padding: 0 1em 0 0; text-align:left\">Variable/Function</th>\n                 <th style=\"border-bottom:1px solid #CDCDCD; padding: 0 1em 0 0; text-align:left\">Description</th></tr><tr><td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top; font-weight:bold; white-space:nowrap; color:green;\">install_datasets()</td>\n                 <td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top;\">A utility function for installing datasets into the current workspace.</td></td></tr><tr><td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top; font-weight:bold; white-space:nowrap; color:green;\">reality_check_install()</td>\n                 <td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top;\">A utility function for validating the install process.</td></td></tr></table></body></html>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<html><body><table style=\"width:100%\">\n            <p style=\"font-size:16px\">The following variables and functions have been defined for you.<br/>Please refer to them in the following instructions.</p><tr><th style=\"border-bottom:1px solid #CDCDCD; padding: 0 1em 0 0; text-align:left\">Variable/Function</th>\n                 <th style=\"border-bottom:1px solid #CDCDCD; padding: 0 1em 0 0; text-align:left\">Description</th></tr><tr><td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top; font-weight:bold; white-space:nowrap; color:green;\">install_datasets()</td>\n                 <td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top;\">A utility function for installing datasets into the current workspace.</td></td></tr><tr><td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top; font-weight:bold; white-space:nowrap; color:green;\">reality_check_install()</td>\n                 <td style=\"border-bottom:1px solid #CDCDCD;; font-size:16px; {padding} vertical-align:top;\">A utility function for validating the install process.</td></td></tr></table></body></html>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["<h2><img src=\"https://files.training.databricks.com/images/105/logo_spark_tiny.png\"> Exercise #1 - Install Datasets</h2>\n\nSimply run the following command to install the capstone's datasets into your workspace."],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"2e8cc60b-06ce-40b8-b9bc-93c092ce33db"}}},{"cell_type":"code","source":["# At any time during this project, you can reinstall the source datasets\n# by setting reinstall=True. These datasets will not be automtically \n# reinstalled when this notebook is re-ran so as to save you time.\ninstall_datasets(reinstall=False)"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"bb63de79-ec62-4105-a703-7763a0094dd8"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"\nThe source directory for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/developer-foundations-capstone/v01/\n\nSkipping install of existing dataset to\ndbfs:/dbacademy/cenz.wong@ekimetrics.com/developer-foundations-capstone/raw\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["\nThe source directory for this dataset is\nwasbs://courseware@dbacademy.blob.core.windows.net/developer-foundations-capstone/v01/\n\nSkipping install of existing dataset to\ndbfs:/dbacademy/cenz.wong@ekimetrics.com/developer-foundations-capstone/raw\n"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["### Reality Check #1\nRun the following command to ensure that you are on track:"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"90513d6c-99c5-425c-9fa9-51bf59bf3b0b"}}},{"cell_type":"code","source":["reality_check_install()"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"7f702df4-b05a-4a9b-9fc7-e26ab3ec63d6"}},"outputs":[{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"Wrote 17 bytes.\n","removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"ansi","arguments":{}}},"output_type":"display_data","data":{"text/plain":["Wrote 17 bytes.\n"]},"transient":null},{"output_type":"display_data","metadata":{"application/vnd.databricks.v1+output":{"datasetInfos":[],"data":"<style>\n  table { text-align: left; border-collapse: collapse; margin: 1em; caption-side: bottom; font-family: Sans-Serif; font-size: 16px}\n  caption { text-align: left; padding: 5px }\n  th, td { border: 1px solid #ddd; padding: 5px }\n  th { background-color: #ddd }\n  .passed { background-color: #97d897 }\n  .failed { background-color: #e2716c }\n  .skipped { background-color: #f9d275 }\n  .results .points { display: none }\n  .results .message { display: block; font-size:smaller; color:gray }\n  .results .note { display: block; font-size:smaller; font-decoration:italics }\n  .results .passed::before  { content: \"Passed\" }\n  .results .failed::before  { content: \"Failed\" }\n  .results .skipped::before { content: \"Skipped\" }\n  .grade .passed  .message:empty::before { content:\"Passed\" }\n  .grade .failed  .message:empty::before { content:\"Failed\" }\n  .grade .skipped .message:empty::before { content:\"Skipped\" }\n</style>\n<table class='results'>\n  <tr><th class='points'>Points</th><th class='test'>Test</th><th class='result'>Result</th></tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Using DBR 9.1 & Proper Cluster Configuration\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Valid Registration ID\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 3 files, found 3 in /\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 or more files, found 3 in /_meta\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 files, found 2 in /products\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 files, found 2 in /orders\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 3 files, found 3 in /orders/batch\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 20 files, found 20 in /orders/stream\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    All datasets were installed succesfully!\n  </td>\n  <td class='result passed'></td>\n</tr>\n  <caption class='points'>Score: 9</caption>\n</table>","textData":null,"removedWidgets":[],"addedWidgets":{},"metadata":{},"type":"htmlSandbox","arguments":{}}},"output_type":"display_data","data":{"text/html":["<style>\n  table { text-align: left; border-collapse: collapse; margin: 1em; caption-side: bottom; font-family: Sans-Serif; font-size: 16px}\n  caption { text-align: left; padding: 5px }\n  th, td { border: 1px solid #ddd; padding: 5px }\n  th { background-color: #ddd }\n  .passed { background-color: #97d897 }\n  .failed { background-color: #e2716c }\n  .skipped { background-color: #f9d275 }\n  .results .points { display: none }\n  .results .message { display: block; font-size:smaller; color:gray }\n  .results .note { display: block; font-size:smaller; font-decoration:italics }\n  .results .passed::before  { content: \"Passed\" }\n  .results .failed::before  { content: \"Failed\" }\n  .results .skipped::before { content: \"Skipped\" }\n  .grade .passed  .message:empty::before { content:\"Passed\" }\n  .grade .failed  .message:empty::before { content:\"Failed\" }\n  .grade .skipped .message:empty::before { content:\"Skipped\" }\n</style>\n<table class='results'>\n  <tr><th class='points'>Points</th><th class='test'>Test</th><th class='result'>Result</th></tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Using DBR 9.1 & Proper Cluster Configuration\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Valid Registration ID\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 3 files, found 3 in /\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 or more files, found 3 in /_meta\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 files, found 2 in /products\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 2 files, found 2 in /orders\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 3 files, found 3 in /orders/batch\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    Expected 20 files, found 20 in /orders/stream\n  </td>\n  <td class='result passed'></td>\n</tr>\n<tr>\n  <td class='points'>1</td>\n  <td class='test'>\n    All datasets were installed succesfully!\n  </td>\n  <td class='result passed'></td>\n</tr>\n  <caption class='points'>Score: 9</caption>\n</table>"]},"transient":null}],"execution_count":0},{"cell_type":"markdown","source":["-sandbox\n&copy; 2021 Databricks, Inc. All rights reserved.<br/>\nApache, Apache Spark, Spark and the Spark logo are trademarks of the <a href=\"http://www.apache.org/\">Apache Software Foundation</a>.<br/>\n<br/>\n<a href=\"https://databricks.com/privacy-policy\">Privacy Policy</a> | <a href=\"https://databricks.com/terms-of-use\">Terms of Use</a> | <a href=\"http://help.databricks.com/\">Support</a>"],"metadata":{"application/vnd.databricks.v1+cell":{"title":"","showTitle":false,"inputWidgets":{},"nuid":"beb3ad19-8f86-4525-9fd0-61d5ad99e28e"}}}],"metadata":{"application/vnd.databricks.v1+notebook":{"notebookName":"Exercise 01 - Overview and Install","dashboards":[],"notebookMetadata":{"pythonIndentUnit":2},"language":"python","widgets":{},"notebookOrigID":2530773532817439}},"nbformat":4,"nbformat_minor":0}
